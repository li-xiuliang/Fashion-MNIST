{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用CNN 做分类\n",
    "---\n",
    "在这个notebook中，我们要对一个CNN进行定义**并训练**，使其学会对 [Fashion-MNIST 数据库](https://github.com/zalandoresearch/fashion-mnist)的图像进行分类。\n",
    "\n",
    "### 加载[数据](http://pytorch.org/docs/master/torchvision/datasets.html)\n",
    "\n",
    "在这个单元格中，我们要加载FashionMNIST类中的**训练与测试**数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our basic libraries\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# data loading and transforming\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors for input into a CNN\n",
    "\n",
    "## Define a transform to read the data in as a tensor\n",
    "data_transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = FashionMNIST(root='./data', train=True,\n",
    "                                   download=True, transform=data_transform)\n",
    "\n",
    "test_data = FashionMNIST(root='./data', train=False,\n",
    "                                  download=True, transform=data_transform)\n",
    "\n",
    "\n",
    "# Print out some stats about the training and test data\n",
    "print('Train data, number of images: ', len(train_data))\n",
    "print('Test data, number of images: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders, set the batch_size\n",
    "## TODO: you can try changing the batch_size to be larger or smaller\n",
    "## when you get to training your network, see how batch_size affects the loss\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将一些训练数据可视化\n",
    "\n",
    " 该单元格会遍历该训练数据集，并使用`dataiter.next()`加载一个随机批次的图像/标签数据。然后，它会在`2 x batch_size/2`网格中将这批图像和标签可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络架构\n",
    "\n",
    " [这里](http://pytorch.org/docs/master/nn.html)记录了构成任何一种神经网络所需的各个层。对于卷积神经网络，我们将使用下列几个简单的层：\n",
    "* 卷积层\n",
    "* 最大池化层\n",
    "* 全连接层（线性层）\n",
    "\n",
    "此外，我们还建议你考虑添加 [dropout 层](http://pytorch.org/docs/stable/nn.html#dropout)，避免过度拟合此数据。\n",
    "\n",
    "---\n",
    "\n",
    "要在PyTorch中定义一个神经网络，你可以选择在函数 `__init__`中定义一个模型的各个层，并定义一个网络的前馈行为，该网络会在函数`forward`中使用这些初始化的层，而该函数会接收输入图像张量`x`。此Net类的结构如下所示，并由你来填写。\n",
    "\n",
    "注意：在训练期间，PyTorch将能够通过跟踪该网络的前馈行为并使用autograd来计算该网络中权重的更新来执行反向传播。\n",
    "\n",
    "#### 在` __init__`中定义各层\n",
    "提醒一下，卷积层/池化层在`__init__`中可以像这样定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 input image channel (for grayscale images), 32 output channels/feature maps, 3x3 square convolution kernel\n",
    "self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "\n",
    "# maxpool that uses a square window of kernel_size=2, stride=2\n",
    "self.pool = nn.MaxPool2d(2, 2)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 引用`forward` 中的层\n",
    "然后在这样的`forward`函数中引用，其中conv1层在应用最大池化层之前应用了一个ReLu激活函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.pool(F.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，你必须要做的是，要把所有具有可训练权重的层放置在`__init__`函数中，例如卷积层，并在`forward`函数中引用它们。所有始终以相同方式运行的层或函数，例如预定义的激活函数，都可能出现在`__init__`或`forward`函数中。实际上，你之后会经常看到在`__init__`中定义的卷积层/池化层和在`forward`中定义的激活层。\n",
    "\n",
    "#### 卷积层\n",
    "你已经定义了第一个卷积层。这个卷积层在用3x3滤镜对图像进行卷积处理之后，会输入1通道的（灰度）图像并输出10个特征图。\n",
    "\n",
    "#### 扁平化\n",
    "\n",
    "回想一下，要从卷积层/池化层的输出移动到线性层（即全连接层），必须先将提取的特征扁平化为矢量。如果你使用过深度学习库Keras，可能已经对`Flatten()`有所了解。此外，在PyTorch中，你可以使用`x = x.view(x.size(0), -1)`，将输入 `x`扁平化。\n",
    "\n",
    "### TODO: 定义其余的层\n",
    "\n",
    "下面，你可以选择在此网络中定义其他的层，具体取决于你。在这里，我们有一些建议，但你可以根据需要自行更改架构和参数。\n",
    "\n",
    "建议与提示：\n",
    "* 至少使用两个卷积层\n",
    "* 输出必须是一个包含10个输出的线性层（对于10类服装的例子来说）\n",
    "* 使用一个dropout层，避免过度拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input image channel (grayscale), 10 output channels/feature maps\n",
    "        # 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        \n",
    "        ## TODO: Define the rest of the layers:\n",
    "        # include another conv layer, maxpooling layers, and linear layers\n",
    "        # also consider adding a dropout layer to avoid overfitting\n",
    "        \n",
    "\n",
    "    ## TODO: define the feedforward behavior\n",
    "    def forward(self, x):\n",
    "        # one activated conv layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # final output\n",
    "        return x\n",
    "\n",
    "# instantiate and print your Net\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 定义损失函数与优化程序\n",
    "\n",
    "请通过阅读这份在线文档，了解有关 [损失函数](http://pytorch.org/docs/master/nn.html#loss-functions) 和 [优化程序](http://pytorch.org/docs/master/optim.html)的更多信息。\n",
    "\n",
    "请注意，对于像这样的分类问题，通常要使用交叉熵损失，可以在以下代码中这样定义：`criterion = nn.CrossEntropyLoss()`。 PyTorch还包括一些标准的随机优化程序，如随机梯度下降和Adam。我们建议你尝试不同的优化程序，看一看你的模型在训练时对这些不同的优化程序有怎样不同的反应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "## TODO: specify loss function (try categorical cross-entropy)\n",
    "criterion = None\n",
    "\n",
    "## TODO: specify optimizer \n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于准确度的说明\n",
    "\n",
    "在训练**之前和之后**，要查看该网络的准确度。通过查看准确度，你可以真正看到该神经网络已经学到了什么技能。在下一个单元格中，让我们来看看未经训练的网络的准确度是多少。我们预计它大约为10％，这与我们猜测的所有10个类的准确度相同。\n",
    "\n",
    "#### 变量\n",
    "\n",
    "在模型处理一个输入张量之前，它必须包装在一个Variable包装器中。这个包装器使PyTorch能够自动跟踪该输入在通过该神经网络时发生的变化，并自动计算反向传播所需的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Calculate accuracy before training\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through test dataset\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    # warp input images in a Variable wrapper\n",
    "    images = Variable(images)\n",
    "\n",
    "    # forward pass to get outputs\n",
    "    # the outputs are a series of class scores\n",
    "    outputs = net(images)\n",
    "\n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # count up total number of correct labels\n",
    "    # for which the predicted and true labels are equal\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = 100.0 * correct.item() / total\n",
    "\n",
    "# print it out!\n",
    "print('Accuracy before training: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练网络\n",
    "\n",
    "下面，我们已经定义了一个`train`函数，它需要输入多个epoch才能进行训练。其中，epoch数是指神经网络在训练数据集中循环的次数。\n",
    "\n",
    "以下是此训练函数在训练数据集上迭代时需要执行的步骤：\n",
    "\n",
    "1. 包装变量中的所有张量\n",
    "2. 为正向传递准备零点的梯度\n",
    "3. 通过网络传递输入（正向传递）\n",
    "4. 计算损失（预测类与正确标签的距离）\n",
    "5. 将梯度传播回网络参数（反向传递）\n",
    "6. 更新权重（参数更新）\n",
    "7. 输出计算出的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in a torch Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)        \n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if batch_i % 1000 == 999:    # print every 1000 mini-batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of epochs to train for\n",
    "n_epochs = 5 # start small to see if your model works, initially\n",
    "\n",
    "# call train\n",
    "train(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对已训练的网络进行测试\n",
    "\n",
    "只要你对模型的损失减少感到满意，就可以进行最后一步了，那就是测试！\n",
    "\n",
    "现在，你必须要使用之前从未见过的数据集测试这个已训练的模型，从而查看它是否能够很好地概括并准确地对这个新数据集进行分类。对于包含许多预处理训练图像的FashionMNIST，一个好的模型在该测试数据集上的**准确度应该达到85％以上**。如果未达到此值，请尝试使用更多epoch进行训练，调整超参数，或添加/减少CNN中的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for batch_i, data in enumerate(test_loader):\n",
    "    \n",
    "    # get the input images and their corresponding labels\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # wrap them in a torch Variable\n",
    "    # volatile means we do not have to track how the inputs change\n",
    "    inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "    \n",
    "    # forward pass to get outputs\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "            \n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((torch.ones(1) / (batch_i + 1)) * (loss.data - test_loss))\n",
    "    \n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n",
    "    \n",
    "    # calculate test accuracy for *each* object class\n",
    "    for i in range(batch_size):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将样本测试结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "# get predictions\n",
    "preds = np.squeeze(net(Variable(images, volatile=True)).data.max(1, keepdim=True)[1].numpy())\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：你的模型存在哪些缺点？ 在未来的迭代中，你会如何改进它们？\n",
    "\n",
    "**答案**: 请双击这里并写下你的答案。\n",
    "\n",
    "### 将表现最好的模型保存起来\n",
    "\n",
    "如果决定了使用哪个网络架构，并且在训练后对模型的测试准确度感到满意，就可以将它保持下来，便于之后做参考，并使用它在另一个分类任务中利用之后的数据做比较！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: change the model_name to something uniqe for any new model\n",
    "## you wish to save, this will save it in the saved_models directory\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'model_1.pt'\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "# when you're ready, un-comment the line below\n",
    "# torch.save(net.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载已训练并保存的模型\n",
    "\n",
    "要实例化训练的模型，首先要实例化一个新的`Net()`，然后使用上面保存步骤中保存的参数字典对其进行初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your Net\n",
    "# this refers to your Net class defined above\n",
    "net = Net()\n",
    "\n",
    "# load the net parameters by name\n",
    "# uncomment and write the name of a saved model\n",
    "#net.load_state_dict(torch.load('saved_models/model_1.pt'))\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Once you've loaded a specific model in, you can then \n",
    "# us it or further analyze it! \n",
    "# This will be especialy useful for feature visualization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
